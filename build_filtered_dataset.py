#!/usr/bin/env python3
"""
íŠ¹ì • í´ë˜ìŠ¤ë§Œ í•„í„°ë§í•˜ì—¬ YOLO ì„¸ê·¸ë©˜í…Œì´ì…˜ ë°ì´í„°ì…‹ ìƒì„± ìŠ¤í¬ë¦½íŠ¸

ì‚¬ìš©ë²•:
    python3 build_filtered_dataset.py --project-dir /path/to/project \
                                       --classes slime ì†Œì‹¤ì  \
                                       --output-dir ./dataset_slime_vanishing \
                                       --split 0.7,0.15,0.15
"""

import argparse
import json
import shutil
import random
from pathlib import Path
from datetime import datetime
import cv2
import numpy as np


def parse_args():
    parser = argparse.ArgumentParser(description='íŠ¹ì • í´ë˜ìŠ¤ë§Œ í•„í„°ë§í•˜ì—¬ YOLO ë°ì´í„°ì…‹ ìƒì„±')
    parser.add_argument('--project-dir', type=str, required=True,
                        help='í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--classes', nargs='+', required=True,
                        help='ì¶”ì¶œí•  í´ë˜ìŠ¤ ëª©ë¡ (ì˜ˆ: slime ì†Œì‹¤ì )')
    parser.add_argument('--output-dir', type=str, default='filtered_dataset',
                        help='ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--split', type=str, default='0.7,0.15,0.15',
                        help='Train/Val/Test ë¹„ìœ¨ (ê¸°ë³¸: 0.7,0.15,0.15)')
    parser.add_argument('--videos-web-dir', type=str,
                        default='/home/intu/nas2_kwater/Videos_web',
                        help='ì›¹ í˜¸í™˜ ë¹„ë””ì˜¤ ë””ë ‰í† ë¦¬')
    return parser.parse_args()


def load_project_config(project_dir):
    """í”„ë¡œì íŠ¸ ì„¤ì • ë¡œë“œ"""
    project_file = project_dir / 'project.json'
    if not project_file.exists():
        raise FileNotFoundError(f"project.json not found in {project_dir}")

    with open(project_file, 'r', encoding='utf-8') as f:
        return json.load(f)


def find_web_video_path(original_path, videos_web_dir):
    """ì›¹ í˜¸í™˜ ë¹„ë””ì˜¤ ê²½ë¡œ ì°¾ê¸°"""
    original_path = Path(original_path)

    # SAHARA í´ë”ì¸ ê²½ìš°
    if 'SAHARA' in str(original_path):
        parts = list(original_path.parts)
        sahara_idx = parts.index('SAHARA')
        relative_path = Path(*parts[sahara_idx+1:])
        web_path = Path(videos_web_dir) / 'SAHARA' / relative_path
        web_path = web_path.with_suffix('.mp4')
    # ê´€ë‚´ì‹œê²½ì˜ìƒ í´ë”ì¸ ê²½ìš°
    elif 'ê´€ë‚´ì‹œê²½ì˜ìƒ' in str(original_path):
        parts = list(original_path.parts)
        kwan_idx = parts.index('ê´€ë‚´ì‹œê²½ì˜ìƒ')
        relative_path = Path(*parts[kwan_idx+1:])
        web_path = Path(videos_web_dir) / 'ê´€ë‚´ì‹œê²½ì˜ìƒ' / relative_path
        web_path = web_path.with_suffix('.mp4')
    else:
        # ê·¸ëŒ€ë¡œ ì‚¬ìš©
        web_path = original_path

    return web_path


def extract_frame(video_path, frame_number):
    """ë¹„ë””ì˜¤ì—ì„œ íŠ¹ì • í”„ë ˆì„ ì¶”ì¶œ"""
    cap = cv2.VideoCapture(str(video_path))
    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)
    ret, frame = cap.read()
    cap.release()

    if not ret:
        return None
    return frame


def polygon_to_yolo(points, img_width, img_height):
    """í´ë¦¬ê³¤ ì¢Œí‘œë¥¼ YOLO ì„¸ê·¸ë©˜í…Œì´ì…˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    normalized_points = []
    for point in points:
        x = point['x'] / img_width
        y = point['y'] / img_height
        # 0-1 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        x = max(0.0, min(1.0, x))
        y = max(0.0, min(1.0, y))
        normalized_points.append(f"{x:.6f} {y:.6f}")

    return ' '.join(normalized_points)


def collect_filtered_annotations(project_dir, target_classes, videos_web_dir):
    """í•„í„°ë§ëœ ì–´ë…¸í…Œì´ì…˜ ìˆ˜ì§‘"""
    print(f"\n{'='*80}")
    print(f"ğŸ” ì–´ë…¸í…Œì´ì…˜ ìˆ˜ì§‘ ì¤‘...")
    print(f"{'='*80}")
    print(f"íƒ€ê²Ÿ í´ë˜ìŠ¤: {', '.join(target_classes)}")

    project_config = load_project_config(project_dir)
    annotations_dir = project_dir / 'annotations'

    # í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘ (í•œê¸€/ì˜ë¬¸ ëª¨ë‘ ì§€ì›)
    class_aliases = {
        'slime': ['slime', 'slime(ë¬¼ë•Œ)', 'ìŠ¬ë¼ì„(ë¬¼ë•Œ)', 'ìŠ¬ë¼ì„'],
        'ì†Œì‹¤ì ': ['ì†Œì‹¤ì ', 'vanishing_point']
    }

    # íƒ€ê²Ÿ í´ë˜ìŠ¤ë¥¼ í‘œì¤€í™”
    target_class_set = set()
    for tc in target_classes:
        if tc in class_aliases:
            target_class_set.update(class_aliases[tc])
        else:
            target_class_set.add(tc)

    # í´ë˜ìŠ¤ ID ë§¤í•‘ ìƒì„±
    class_to_id = {cls: idx for idx, cls in enumerate(target_classes)}

    collected_frames = []
    stats = {cls: 0 for cls in target_classes}

    # ë¹„ë””ì˜¤ë³„ ê²½ë¡œ ë§¤í•‘
    video_path_map = {}
    for video in project_config.get('videos', []):
        video_id = video['video_id']
        original_path = video['video_path']
        web_path = find_web_video_path(original_path, videos_web_dir)
        video_path_map[video_id] = {
            'web_path': web_path,
            'width': video.get('width', 1920),
            'height': video.get('height', 1080)
        }

    # ì–´ë…¸í…Œì´ì…˜ ìˆ˜ì§‘
    for video_folder in sorted(annotations_dir.iterdir()):
        if not video_folder.is_dir():
            continue

        video_id = video_folder.name

        if video_id not in video_path_map:
            print(f"âš ï¸  ë¹„ë””ì˜¤ ì •ë³´ ì—†ìŒ: {video_id}")
            continue

        video_info = video_path_map[video_id]

        # ë¹„ë””ì˜¤ íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not video_info['web_path'].exists():
            print(f"âš ï¸  ë¹„ë””ì˜¤ íŒŒì¼ ì—†ìŒ: {video_info['web_path']}")
            continue

        # ê° ì‚¬ìš©ìì˜ ì–´ë…¸í…Œì´ì…˜ íŒŒì¼ ì½ê¸°
        for json_file in video_folder.glob('*.json'):
            if 'backup' in json_file.name or 'before_fix' in json_file.name:
                continue

            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                annotations = data.get('annotations', {})

                for frame_num, frame_annos in annotations.items():
                    # ì´ í”„ë ˆì„ì— íƒ€ê²Ÿ í´ë˜ìŠ¤ê°€ ìˆëŠ”ì§€ í™•ì¸
                    filtered_annos = []

                    for anno in frame_annos:
                        label = anno.get('label', '')

                        if label in target_class_set:
                            # ì›ë˜ í´ë˜ìŠ¤ë¡œ ë§¤í•‘ (ì˜ˆ: 'slime(ë¬¼ë•Œ)' -> 'slime')
                            mapped_class = None
                            for original_cls, aliases in class_aliases.items():
                                if label in aliases:
                                    mapped_class = original_cls
                                    break

                            if mapped_class and mapped_class in class_to_id:
                                filtered_annos.append({
                                    'class': mapped_class,
                                    'class_id': class_to_id[mapped_class],
                                    'points': anno.get('polygon', [])
                                })
                                stats[mapped_class] += 1

                    # íƒ€ê²Ÿ í´ë˜ìŠ¤ê°€ ìˆëŠ” í”„ë ˆì„ë§Œ ìˆ˜ì§‘
                    if filtered_annos:
                        collected_frames.append({
                            'video_id': video_id,
                            'video_path': video_info['web_path'],
                            'frame_number': int(frame_num),
                            'width': video_info['width'],
                            'height': video_info['height'],
                            'annotations': filtered_annos,
                            'user': json_file.stem
                        })

            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜ ({json_file.name}): {e}")

    print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ:")
    print(f"   - ì´ í”„ë ˆì„: {len(collected_frames)}ê°œ")
    for cls, count in stats.items():
        print(f"   - {cls}: {count}ê°œ")

    return collected_frames, class_to_id


def split_dataset(frames, split_ratio):
    """ë°ì´í„°ì…‹ì„ train/val/testë¡œ ë¶„í• """
    train_ratio, val_ratio, test_ratio = map(float, split_ratio.split(','))
    total = train_ratio + val_ratio + test_ratio
    train_ratio /= total
    val_ratio /= total

    # ì…”í”Œ
    random.shuffle(frames)

    total_frames = len(frames)
    train_end = int(total_frames * train_ratio)
    val_end = train_end + int(total_frames * val_ratio)

    return {
        'train': frames[:train_end],
        'val': frames[train_end:val_end],
        'test': frames[val_end:]
    }


def build_dataset(frames_dict, class_mapping, output_dir):
    """YOLO ë°ì´í„°ì…‹ ë¹Œë“œ"""
    output_path = Path(output_dir)

    # ë””ë ‰í† ë¦¬ ìƒì„±
    for split in ['train', 'val', 'test']:
        (output_path / split / 'images').mkdir(parents=True, exist_ok=True)
        (output_path / split / 'labels').mkdir(parents=True, exist_ok=True)

    print(f"\n{'='*80}")
    print(f"ğŸ—ï¸  ë°ì´í„°ì…‹ ë¹Œë“œ ì¤‘...")
    print(f"{'='*80}")

    total_saved = 0

    for split, frames in frames_dict.items():
        print(f"\nğŸ“¦ {split.upper()} ì„¸íŠ¸ ìƒì„± ì¤‘... ({len(frames)}ê°œ í”„ë ˆì„)")

        saved_count = 0
        for idx, frame_data in enumerate(frames):
            video_path = frame_data['video_path']
            frame_num = frame_data['frame_number']
            width = frame_data['width']
            height = frame_data['height']
            annotations = frame_data['annotations']

            # í”„ë ˆì„ ì¶”ì¶œ
            frame_img = extract_frame(video_path, frame_num)
            if frame_img is None:
                print(f"âš ï¸  í”„ë ˆì„ ì¶”ì¶œ ì‹¤íŒ¨: {video_path} #{frame_num}")
                continue

            # íŒŒì¼ëª… ìƒì„±
            video_id = frame_data['video_id']
            filename = f"{video_id}_frame_{frame_num:06d}"

            # ì´ë¯¸ì§€ ì €ì¥
            img_path = output_path / split / 'images' / f"{filename}.jpg"
            cv2.imwrite(str(img_path), frame_img)

            # ë¼ë²¨ íŒŒì¼ ìƒì„±
            label_path = output_path / split / 'labels' / f"{filename}.txt"
            with open(label_path, 'w') as f:
                for anno in annotations:
                    class_id = anno['class_id']
                    points = anno['points']

                    if len(points) < 3:  # ìµœì†Œ 3ê°œ ì  í•„ìš”
                        continue

                    yolo_coords = polygon_to_yolo(points, width, height)
                    f.write(f"{class_id} {yolo_coords}\n")

            saved_count += 1

            if (idx + 1) % 50 == 0:
                print(f"   ì§„í–‰: {idx + 1}/{len(frames)} í”„ë ˆì„")

        print(f"   âœ… {split}: {saved_count}ê°œ ì €ì¥ë¨")
        total_saved += saved_count

    # data.yaml ìƒì„±
    yaml_content = f"""# YOLO Segmentation Dataset
# Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

path: {output_path.absolute()}
train: train/images
val: val/images
test: test/images

nc: {len(class_mapping)}
names: {list(class_mapping.keys())}
"""

    yaml_path = output_path / 'data.yaml'
    with open(yaml_path, 'w', encoding='utf-8') as f:
        f.write(yaml_content)

    print(f"\n{'='*80}")
    print(f"âœ… ë°ì´í„°ì…‹ ë¹Œë“œ ì™„ë£Œ!")
    print(f"{'='*80}")
    print(f"ì´ ì €ì¥ëœ ì´ë¯¸ì§€: {total_saved}ê°œ")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_path.absolute()}")
    print(f"ì„¤ì • íŒŒì¼: {yaml_path}")
    print(f"{'='*80}\n")

    return total_saved


def main():
    args = parse_args()

    print(f"\n{'='*80}")
    print(f"ğŸ¯ YOLO í•„í„°ë§ ë°ì´í„°ì…‹ ë¹Œë”")
    print(f"{'='*80}")
    print(f"í”„ë¡œì íŠ¸: {args.project_dir}")
    print(f"íƒ€ê²Ÿ í´ë˜ìŠ¤: {', '.join(args.classes)}")
    print(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
    print(f"Split ë¹„ìœ¨: {args.split}")

    project_dir = Path(args.project_dir)
    if not project_dir.exists():
        print(f"âŒ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {project_dir}")
        return

    # 1. ì–´ë…¸í…Œì´ì…˜ ìˆ˜ì§‘
    frames, class_mapping = collect_filtered_annotations(
        project_dir,
        args.classes,
        args.videos_web_dir
    )

    if not frames:
        print("\nâŒ íƒ€ê²Ÿ í´ë˜ìŠ¤ì˜ ì–´ë…¸í…Œì´ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ë°ì´í„°ì…‹ ë¶„í• 
    print(f"\n{'='*80}")
    print(f"ğŸ“Š ë°ì´í„°ì…‹ ë¶„í•  ì¤‘...")
    print(f"{'='*80}")
    frames_dict = split_dataset(frames, args.split)

    for split, split_frames in frames_dict.items():
        print(f"   {split}: {len(split_frames)}ê°œ")

    # 3. ë°ì´í„°ì…‹ ë¹Œë“œ
    total_saved = build_dataset(frames_dict, class_mapping, args.output_dir)

    if total_saved > 0:
        print(f"\nâœ¨ ì„±ê³µì ìœ¼ë¡œ {total_saved}ê°œì˜ ì´ë¯¸ì§€ë¡œ ë°ì´í„°ì…‹ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤!")
        print(f"ğŸ“ {Path(args.output_dir).absolute()}/data.yaml ì„ í™•ì¸í•˜ì„¸ìš”.\n")
    else:
        print(f"\nâŒ ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨\n")


if __name__ == '__main__':
    random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•´
    main()
